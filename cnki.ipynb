{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DrissionPage import ChromiumPage\n",
    "from urllib.parse import unquote\n",
    "import time\n",
    "from lxml import  etree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = ChromiumPage()\n",
    "page.listen.start(['group/singleresult', 'kns8s/brief/grid'])\n",
    "page.get('https://kns.cnki.net/kns8s/AdvSearch')\n",
    "print('等待搜索')\n",
    "while True:\n",
    "    arti = page.eles('xpath://*[@id=\"gridTable\"]/div/div/div/table/tbody/tr[1]', timeout=1)\n",
    "    if arti:\n",
    "        break\n",
    "print('搜索完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取各分类数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count= {}\n",
    "category_list= []\n",
    "doctype_menus = page.eles('xpath://ul[contains(@class,\"doctype-menu\")]//a[@name=\"classify\"]', timeout=2)\n",
    "for i in doctype_menus:\n",
    "    t = i.ele('xpath://span').text\n",
    "    n = i.ele('xpath://em').text\n",
    "    category_count[t] = n\n",
    "    print(t,n)\n",
    "    category_list.append({'value':n, 'name':t})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "侧边计数点击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divGroups = page.eles('xpath://div[@id=\"divGroup\"]/dl/dt[@class=\"tit\"]/b')\n",
    "for i in divGroups:\n",
    "    print(i.text)\n",
    "    i.click(by_js=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "翻页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    PageNext = page.ele('#PageNext', timeout=2)\n",
    "    if PageNext:\n",
    "        PageNext.click()\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取监听数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_count = {} # 左侧分类计数\n",
    "subject_count_list = {} # 左侧分类计数列表\n",
    "articles = [] # 文章列表\n",
    "\n",
    "for i in page.listen.steps(timeout=2):\n",
    "    url = i.request.url\n",
    "    data = i.response.body\n",
    "    if 'kns8s/brief/grid' in url: # 主体内容\n",
    "        h = etree.HTML(data)\n",
    "        items = h.xpath('//table[@class=\"result-table-list\"]/tbody/tr')\n",
    "        for item in items:\n",
    "            temp = {}\n",
    "            temp['url'] = item.xpath('./td[@class=\"name\"]//a/@href')[0]\n",
    "            temp['title'] = item.xpath('./td[@class=\"name\"]//a/text()')[0]\n",
    "            \n",
    "            \n",
    "            authors = []\n",
    "            author = item.xpath('./td[@class=\"author\"]//a')\n",
    "            for i in author:\n",
    "                author_url = i.xpath('./@href')[0]\n",
    "                author_name = i.xpath('.//text()')[0]\n",
    "                if 'javascript' in author_url:\n",
    "                    authors = [{'name': i, 'url': None}  for i in ''.join(i.xpath('.//text()')).split('，') ]\n",
    "                else:\n",
    "                    authors.append({'name': author_name, 'url': author_url})\n",
    "                \n",
    "            temp['author'] = item.xpath('./td[@class=\"author\"]//a//text()')\n",
    "            temp['source'] = item.xpath('./td[@class=\"source\"]//a/text()')[0]\n",
    "            temp['date'] = item.xpath('./td[@class=\"date\"]//text()')[0] # 时间\n",
    "            temp['data'] = item.xpath('./td[@class=\"data\"]//span/text()') # 数据库\n",
    "            temp['data'] = temp['data'][0] if temp['data'] else None\n",
    "            temp['quote'] = item.xpath('./td[@class=\"quote\"]//span/text()')\n",
    "            temp['quote'] = temp['quote'][0] if temp['quote'] else None\n",
    "            temp['download'] = item.xpath('./td[@class=\"date\"]//a/text()') # 下载\n",
    "            temp['download'] = temp['download'][0] if temp['download'] else None\n",
    "            articles.append(temp) \n",
    "    elif 'group/singleresult' in url: # 左侧计数信息\n",
    "        print(data)\n",
    "        h = etree.HTML(data)\n",
    "        tit = h.xpath('//dd/@tit')[0]\n",
    "        items = h.xpath('//li')\n",
    "        temp = {}\n",
    "        temp_list = []\n",
    "        for item in items:\n",
    "            name = item.xpath('./a/text()')[0]\n",
    "            value = item.xpath('./span/text()')[0]\n",
    "            value = value.replace('(','').replace(')','')\n",
    "            if '万' in value:\n",
    "                value = float(value.replace('万',''))*10000\n",
    "            temp[name] = value\n",
    "            temp_list.append({'name':name,'value':value})\n",
    "        subject_count[tit] = temp\n",
    "        subject_count_list[tit] = temp_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计量可视化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anisys_all = page.ele('xpath://li[@id=\"anisys_all\"]')\n",
    "anisys_all.click(by_js=True)\n",
    "time.sleep(1)\n",
    "anisys_tab = page.get_tab(title='计量可视化分析')\n",
    "anisys_tab.listen.start(['getGroupData'])\n",
    "time.sleep(1)\n",
    "anisys_tab.refresh()\n",
    "time.sleep(1)\n",
    "anisys_tab.refresh()\n",
    "time.sleep(1)\n",
    "anisys_data = {}\n",
    "for i in anisys_tab.listen.steps(timeout=2):\n",
    "    url = i.request.url\n",
    "    postData =unquote(i.request.postData) \n",
    "    data = i.response.body\n",
    "    for i in data:\n",
    "        if 'y' in i:\n",
    "            i['value'] = i['y']\n",
    "    if 'getGroupData' in url:\n",
    "        anisys_data[postData.split('&')[3].split('=')[1]] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取用户信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    author_tab = page.get_tab(title='作者')\n",
    "except:\n",
    "    author_tab = page.new_tab()\n",
    "author_tab.listen.start('experts')\n",
    "author_tab.get('https://kns.cnki.net/kcms2/author/detail?v=Nyg97wmOeE7B23i7DB-Wq31WeINiF06xmgTmoBoMx0sXtkAxZJoDot5NMIzLQfaSh-HhvifSwPKRh-ZDhdGYQAYlnz35f_S2ZwyPVVXbED3805Z7yyKKIC4yiYR3YObe&uniplatform=NZKPT&language=CHS')\n",
    "for i in range(50):\n",
    "    time.sleep(1)\n",
    "    author_tab.scroll.to_bottom()\n",
    "    recvideotitle = author_tab.ele('xpath://h2[@id=\"recvideotitle\"]', timeout=0.5)\n",
    "    if recvideotitle:\n",
    "        print('到底了')\n",
    "        break\n",
    "\n",
    "author_data = {}\n",
    "author_data['names'] = {\n",
    "    'detail': '作者详情',\n",
    "    'query': '同名作者',\n",
    "    'domains': '作者关注领域',\n",
    "    'quote_rank': '最高被引',\n",
    "    'download_rank': '最高下载',\n",
    "    'journal': '发表在期刊的文献',\n",
    "    'meeting': '发表在会议的文献',\n",
    "    'thesis': '发表在硕博的文献',\n",
    "    'newspaper': '发表在报纸的文献',\n",
    "    'max_pubjournals': '最高发表期刊',\n",
    "    'max_pubconferences': '最高发表会议',\n",
    "    'max_pubdegreeunits': '最高发表硕博',\n",
    "    'max_pubnewspapers': '最高发表报纸',\n",
    "    'overReferences':'曾参考文献',\n",
    "    'tutors':'导师',\n",
    "    'coauthors':'合作者',\n",
    "    'funds': '作者基金',\n",
    "    'videos': '视频'\n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "for i in author_tab.listen.steps(timeout=2):\n",
    "    url = i.request.url\n",
    "    t = url.split('?')[0].split('/')[-1]\n",
    "    data = i.response.body\n",
    "    params = i.request.params\n",
    "    if 'detail' in url: # 作者详情\n",
    "        author_data['detail'] = data.get('data',{})\n",
    "    elif 'query' in url: # 同名作者\n",
    "        author_data['query'] = data.get('data',{})\n",
    "    elif 'domains' in url: #作者关注领域\n",
    "        author_data['domains'] = data.get('data',{}).get('data',{})\n",
    "    elif 'resources' in url: \n",
    "        \n",
    "        resource = params.get('resource')\n",
    "        sequence = params.get('sequence')\n",
    "        if resource == 'SCDB' and sequence == 'CF': # 最高被引\n",
    "            author_data['quote_rank'] = data.get('data',{}).get('data',{})\n",
    "        elif resource == 'SCDB' and sequence == 'DFR': # 最高下载\n",
    "            author_data['download_rank'] = data.get('data',{}).get('data',{})\n",
    "        elif resource == 'CJFD' and sequence == 'PT': # 发表在期刊的文献\n",
    "            author_data['journal'] = data.get('data',{}).get('data',{})\n",
    "        elif resource == 'CIPD' and sequence == 'PT': # 发表在会议的文献\n",
    "            author_data['meeting'] = data.get('data',{}).get('data',{})\n",
    "        elif resource == 'CDMD' and sequence == 'PT': # 发表在硕博的文献\n",
    "            author_data['thesis'] = data.get('data',{}).get('data',{})\n",
    "        elif resource == 'CCND' and sequence == 'PT': # 发表在报纸的文献\n",
    "            author_data['newspaper'] = data.get('data',{}).get('data',{})\n",
    "    elif 'publications' in url: # 发表文献\n",
    "        if params.get('type') == 'journals': # 最高发表期刊\n",
    "            author_data['max_pubjournals'] = data.get('data',{}).get('data',{})\n",
    "        elif params.get('type') == 'conferences': # 最高发表会议\n",
    "            author_data['max_pubconferences'] = data.get('data',{}).get('data',{})\n",
    "        elif params.get('type') == 'degreeunits': # 最高发表在硕博的文献的单位\n",
    "            author_data['max_pubdegreeunits'] = data.get('data',{}).get('data',{})\n",
    "        elif params.get('type') == 'newspapers': # 最高发表在报纸的文献的单位\n",
    "            author_data['max_pubnewspapers'] = data.get('data',{}).get('data',{})\n",
    "    elif 'literature/overReferences' in url: # 曾参考文献\n",
    "        author_data['overReferences'] = data.get('data',{}).get('data',{})\n",
    "    elif 'tutors' in url: # 作者导师\n",
    "        author_data['tutors'] = data.get('data',[])\n",
    "    elif 'coauthors' in url: # 作者合作\n",
    "        author_data['coauthors'] = data.get('data',{}).get('data',{})\n",
    "    elif 'funds' in url: # 作者基金\n",
    "        author_data['funds'] = data.get('data',{}).get('data',{})\n",
    "    elif 'videos' in url: # 作者视频\n",
    "        author_data['videos'] = data.get('data',{}).get('data',{})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取所有文章信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    article_tab = page.get(url='article/abstract')\n",
    "except:\n",
    "    article_tab = page.new_tab()\n",
    "for index,a in enumerate(articles):\n",
    "    if not a.get('detail'):\n",
    "        time.sleep(3)\n",
    "        article_tab.get(a['url'])\n",
    "        print(a['title'], a['url'])\n",
    "        desc = article_tab.eles('xpath://div[@class=\"doc\"]//div[@class=\"row\"]')\n",
    "        r = []\n",
    "        for i in (desc):\n",
    "            # print(i.text)\n",
    "            if '摘要' in i.text:\n",
    "                r.append('摘要')\n",
    "                r.append(i.text.split('摘要', 1)[1].strip())\n",
    "            elif '：' in i.text or '\\n' in i.text:\n",
    "                for j in i.text.split('：'):\n",
    "                    for k in j.split('\\n'):\n",
    "                        if k != '':\n",
    "                            r.append (k.strip())\n",
    "            else:\n",
    "                r.append(i.text.strip())\n",
    "        r = {r[i]:r[i+1] for i in range(0,len(r),2)}\n",
    "        print(r)\n",
    "        articles[index]['detail'] = r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    'category_count': category_list,\n",
    "    'subject_count': subject_count_list,\n",
    "    'funds': [],\n",
    "    'coauthors_nodes': [],\n",
    "    'coauthors_links': [],\n",
    "    'author':{}\n",
    "}\n",
    "metadata = {}\n",
    "metadata['url'] = author_data['detail']['relations'][0]['url']\n",
    "\n",
    "for i in author_data['detail']['metadata']:\n",
    "    metadata[i['name']] = i['value']\n",
    "result['author'] = {\n",
    "    'cnki_id': metadata['CODE'],\n",
    "    'id': metadata['NAME'],\n",
    "    'name': metadata['NAME'],\n",
    "    'url': metadata['url'],\n",
    "}\n",
    "\n",
    "\n",
    "for i in author_data['detail']['metrics']:\n",
    "    result['author'][i['name']] = i['value']\n",
    "for i in author_data['funds']:\n",
    "    result['funds'].append({\n",
    "        'name': i['title'],\n",
    "        'value': i['metrics'][0]['value'],\n",
    "        \"PUC\": i['metrics'][0]['value'],\n",
    "        \"url\": i['relations'][0]['url']\n",
    "    })\n",
    "for i in author_data['coauthors']:\n",
    "    result['coauthors_nodes'].append({\n",
    "        \"cnki_id\":i['id'],\n",
    "        'id': i['title'],\n",
    "        'name': i['title'],\n",
    "        'value': sum(int(j['value']) for j in i['metrics']),\n",
    "        \"PUC\": i['metrics'][0]['value'],\n",
    "        \"DTC\": i['metrics'][1]['value'],\n",
    "        \"CTC\": i['metrics'][2]['value'],\n",
    "        \"url\": i['relations'][0]['url']\n",
    "    })\n",
    "    result['coauthors_links'].append({\n",
    "        \"source\": result['author']['name'],\n",
    "        \"target\": i['title'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open('subject_count.json','w',encoding='utf-8') as f:\n",
    "#     json.dump(subject_count, f, ensure_ascii=False, indent=4)\n",
    "# with open('articles.json','w',encoding='utf-8') as f:\n",
    "#     json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "# with open('anisys_data.json','w',encoding='utf-8') as f:\n",
    "#     json.dump(anisys_data, f, ensure_ascii=False, indent=4)\n",
    "# with open('category_count.json','w',encoding='utf-8') as f:\n",
    "#     json.dump(category_count, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('result_user_subject.json','w',encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
